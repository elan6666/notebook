# ğŸš€ äº‘ç«¯æ·±åº¦å­¦ä¹ è®­ç»ƒå®Œå…¨æŒ‡å—ï¼ˆVSCode + PyTorchç‰ˆï¼‰


---

## 1. äº‘æœåŠ¡å™¨é€‰å‹ä¸ç§Ÿç”¨

### 1.1 ä¸»æµå¹³å°å¯¹æ¯”

è¡¨æ ¼

å¤åˆ¶

|å¹³å°|ä¼˜åŠ¿|é€‚ç”¨åœºæ™¯|ä»·æ ¼åŒºé—´ï¼ˆRTX 3090/å°æ—¶ï¼‰|
|:--|:--|:--|:--|
|**AutoDL**|å›½å†…è®¿é—®å¿«ï¼ŒæŒ‰å°æ—¶è®¡è´¹ï¼Œé•œåƒä¸°å¯Œ|çŸ­æœŸå®éªŒã€å­¦ç”Ÿå…š|Â¥1.5-2.5|
|**é˜¿é‡Œäº‘PAI**|ç”Ÿæ€å®Œæ•´ï¼Œç¨³å®šæ€§é«˜|ä¼ä¸šç”Ÿäº§ç¯å¢ƒ|Â¥3-5|
|**è…¾è®¯äº‘TIå¹³å°**|æ–°ç”¨æˆ·ä¼˜æƒ å¤š|æ–°æ‰‹å…¥é—¨|Â¥2-4|
|**Vast.ai**|å…¨çƒæœ€ä½ä»·|é¢„ç®—æ•æ„Ÿã€èƒ½æŠ˜è…¾|$0.3-0.8|
|**Lambda Cloud**|é¢„è£…ç¯å¢ƒå®Œå–„|å¿«é€Ÿå¯åŠ¨|$1.1-1.5|
|**Google Colab Pro**|é›¶é…ç½®ï¼ŒNotebookå‹å¥½|è½»é‡çº§å®éªŒ|$10/æœˆ|

**æ¨è**ï¼šå›½å†…ç”¨æˆ·é¦–é€‰ **AutoDL**ï¼ˆæ”¯ä»˜å®ä»˜æ¬¾ã€ç½‘ç»œç¨³å®šï¼‰ï¼Œå›½é™…ç”¨æˆ·å¯é€‰ **Vast.ai** æˆ– **Lambda**ã€‚

### 1.2 AutoDLç§Ÿç”¨å®æˆ˜æ­¥éª¤

1. **æ³¨å†Œä¸è®¤è¯**
    
    - è®¿é—® [AutoDLå®˜ç½‘](https://www.autodl.com/)
        
    - å®Œæˆæ‰‹æœºå·ç»‘å®šå’Œå®åè®¤è¯ï¼ˆéœ€èº«ä»½è¯ï¼‰
        
2. **é€‰æ‹©GPUå®ä¾‹**
    
    plain
    
    å¤åˆ¶
    
    ```plain
    ç®—åŠ›å¸‚åœº â†’ é€‰æ‹©åœ°åŒºï¼ˆå»ºè®®é€‰"è¥¿åŒ—åŒº"ä»·æ ¼æœ€ä½ï¼‰
    â†’ ç­›é€‰GPUå‹å·ï¼ˆæ¨èRTX 3090 24Gæˆ–A100 40Gï¼‰
    â†’ é€‰æ‹©åŸºç¡€é•œåƒï¼ˆPyTorch 2.0 + CUDA 11.8ï¼‰
    ```
    
3. **å¼€æœºä¸è·å–è¿æ¥ä¿¡æ¯**
    
    - ç‚¹å‡»"å¼€æœº"åè·å–ï¼š
        
        - `ä¸»æœºåœ°å€`ï¼šä¾‹å¦‚ `connect.westb.seetacloud.com`
            
        - `SSHç«¯å£`ï¼šä¾‹å¦‚ `12345`
            
        - `å¯†ç `ï¼šåˆå§‹åŒ–å¯†ç æˆ–è‡ªå®šä¹‰
            

---

## 2. SSHè¿æ¥ä¸åŸºç¡€é…ç½®

### 2.1 æœ¬åœ°SSHè¿æ¥æµ‹è¯•ï¼ˆWindows/Mac/Linuxï¼‰

bash

å¤åˆ¶

```bash
# æ ¼å¼ï¼šssh -p ç«¯å£ ç”¨æˆ·å@ä¸»æœºåœ°å€
# AutoDLé»˜è®¤ç”¨æˆ·åæ˜¯ root
ssh -p 12345 root@connect.westb.seetacloud.com

# è¾“å…¥å¯†ç ï¼ˆè¾“å…¥æ—¶ä¸æ˜¾ç¤ºå­—ç¬¦ï¼‰
```

**Windowsç”¨æˆ·**ï¼šå»ºè®®ä½¿ç”¨ PowerShell æˆ– Git Bashï¼Œé¿å…CMDç¼–ç é—®é¢˜ã€‚

### 2.2 é…ç½®SSHå…å¯†ç™»å½•ï¼ˆå¿…åšï¼‰

bash

å¤åˆ¶

```bash
# æœ¬åœ°ç»ˆç«¯ç”Ÿæˆå¯†é’¥ï¼ˆå¦‚å·²æœ‰å¯è·³è¿‡ï¼‰
ssh-keygen -t ed25519 -C "your_email@example.com"

# å¤åˆ¶å…¬é’¥åˆ°æœåŠ¡å™¨ï¼ˆå°†~/.ssh/id_ed25519.pubå†…å®¹è¿½åŠ åˆ°æœåŠ¡å™¨ï¼‰
ssh-copy-id -p 12345 root@connect.westb.seetacloud.com
```

### 2.3 æœåŠ¡å™¨åŸºç¡€ç¯å¢ƒæ£€æŸ¥

è¿æ¥åé¦–å…ˆéªŒè¯GPUå’ŒCUDAï¼š

bash

å¤åˆ¶

```bash
# æŸ¥çœ‹GPUçŠ¶æ€
nvidia-smi

# é¢„æœŸè¾“å‡ºåº”æ˜¾ç¤ºï¼š
# - GPUå‹å·ï¼ˆå¦‚RTX 3090ï¼‰
# - CUDAç‰ˆæœ¬ï¼ˆå¦‚12.0ï¼‰
# - æ˜¾å­˜å ç”¨æƒ…å†µ

# æ£€æŸ¥é¢„è£…ç¯å¢ƒ
python --version  # åº”æ˜¾ç¤º3.8æˆ–3.10
conda --version   # æ£€æŸ¥condaæ˜¯å¦å¯ç”¨
```

---

## 3. VSCodeè¿œç¨‹å¼€å‘ç¯å¢ƒæ­å»º

### 3.1 å®‰è£…å¿…è¦æ’ä»¶

åœ¨æœ¬åœ°VSCodeå®‰è£…ï¼š

1. **Remote - SSH**ï¼ˆå¾®è½¯å®˜æ–¹ï¼‰ï¼šè¿œç¨‹è¿æ¥æ ¸å¿ƒ
    
2. **Remote Development**ï¼ˆæ‰©å±•åŒ…ï¼‰ï¼šåŒ…å«SSHã€Containersã€WSL
    
3. **Python**ï¼ˆå¾®è½¯å®˜æ–¹ï¼‰ï¼šPythonè¯­è¨€æ”¯æŒ
    
4. **Jupyter**ï¼ˆå¯é€‰ï¼‰ï¼šNotebookæ”¯æŒ
    

### 3.2 é…ç½®SSHè¿æ¥

1. **æ‰“å¼€SSHé…ç½®æ–‡ä»¶**
    
    - `Ctrl+Shift+P` â†’ è¾“å…¥ `Remote-SSH: Open SSH Configuration File`
        
    - é€‰æ‹© `C:\Users\ä½ çš„ç”¨æˆ·å\.ssh\config`ï¼ˆWindowsï¼‰æˆ– `~/.ssh/config`ï¼ˆMac/Linuxï¼‰
        
2. **æ·»åŠ æœåŠ¡å™¨é…ç½®**
    
    ssh-config
    
    å¤åˆ¶
    
    ```ssh-config
    Host autodl-gpu
        HostName connect.westb.seetacloud.com
        User root
        Port 12345
        IdentityFile ~/.ssh/id_ed25519  # å…å¯†ç™»å½•å¯†é’¥è·¯å¾„
        ServerAliveInterval 60
        ServerAliveCountMax 3
    ```
    
3. **è¿æ¥æœåŠ¡å™¨**
    
    - ç‚¹å‡»VSCodeå·¦ä¸‹è§’ç»¿è‰²å›¾æ ‡ `><`
        
    - é€‰æ‹© `Connect to Host...` â†’ `autodl-gpu`
        
    - é¦–æ¬¡è¿æ¥ä¼šè‡ªåŠ¨åœ¨æœåŠ¡å™¨å®‰è£…VSCode Serverï¼Œç­‰å¾…2-3åˆ†é’Ÿ
        

### 3.3 éªŒè¯è¿œç¨‹ç¯å¢ƒ

æˆåŠŸè¿æ¥åï¼š

- å·¦ä¸‹è§’æ˜¾ç¤º `SSH: autodl-gpu`
    
- æ‰“å¼€ç»ˆç«¯ï¼ˆ`` Ctrl+` ``ï¼‰åº”æ˜¾ç¤ºæœåŠ¡å™¨shell\
    
- æ–‡ä»¶æµè§ˆå™¨æ˜¾ç¤ºçš„æ˜¯æœåŠ¡å™¨æ–‡ä»¶ç³»ç»Ÿ
    

**åˆ›å»ºé¡¹ç›®ç›®å½•**ï¼š

bash

å¤åˆ¶

```bash
mkdir -p ~/projects/drug-repurposing  # æ ¹æ®ä½ çš„è¯ç‰©æ‰°åŠ¨èƒŒæ™¯ä¸¾ä¾‹
cd ~/projects/drug-repurposing
```

---

## 4. PyTorchæ·±åº¦å­¦ä¹ ç¯å¢ƒé…ç½®

### 4.1 åˆ›å»ºéš”ç¦»çš„Condaç¯å¢ƒ

bash

å¤åˆ¶

```bash
# åˆ›å»ºPython 3.10ç¯å¢ƒï¼ˆæ¨èï¼Œå…¼å®¹æ€§å¥½ï¼‰
conda create -n drugdl python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate drugdl

# æ·»åŠ åˆ°.bashrcç¡®ä¿è‡ªåŠ¨æ¿€æ´»ï¼ˆå¯é€‰ï¼‰
echo "conda activate drugdl" >> ~/.bashrc
```

### 4.2 å®‰è£…PyTorchï¼ˆGPUç‰ˆæœ¬ï¼‰

bash

å¤åˆ¶

```bash
# æŸ¥çœ‹CUDAç‰ˆæœ¬ï¼ˆå‡è®¾nvidia-smiæ˜¾ç¤º12.0ï¼‰
# å®‰è£…å¯¹åº”ç‰ˆæœ¬çš„PyTorch 2.1
conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 \
    pytorch-cuda=12.1 -c pytorch -c nvidia

# éªŒè¯å®‰è£…
python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
# åº”è¾“å‡ºï¼š2.1.0 å’Œ True
```

### 4.3 å®‰è£…æ·±åº¦å­¦ä¹ å·¥å…·é“¾

bash

å¤åˆ¶

```bash
# ç§‘å­¦è®¡ç®—ä¸æ•°æ®å¤„ç†
pip install numpy pandas scikit-learn matplotlib seaborn

# æ·±åº¦å­¦ä¹ ç”Ÿæ€
pip install tensorboard wandb timm opencv-python pillow

# è¯ç‰©/ç”Ÿç‰©ä¿¡æ¯å­¦ç›¸å…³ï¼ˆåŸºäºä½ çš„èƒŒæ™¯ï¼‰
pip install rdkit deepchem biopython

# å®ç”¨å·¥å…·
pip install tqdm black isort jupyter ipykernel
```

### 4.4 é…ç½®VSCode Pythonè§£é‡Šå™¨

1. `Ctrl+Shift+P` â†’ `Python: Select Interpreter`
    
2. é€‰æ‹© `~/miniconda3/envs/drugdl/bin/python`
    
3. åˆ›å»ºæµ‹è¯•æ–‡ä»¶ `test_gpu.py`ï¼š
    

Python

å¤åˆ¶

```python
import torch
import torch.nn as nn

print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
print(f"å½“å‰GPU: {torch.cuda.get_device_name(0)}")

# ç®€å•çŸ©é˜µè¿ç®—æµ‹è¯•
x = torch.randn(1000, 1000).cuda()
y = torch.randn(1000, 1000).cuda()
z = torch.matmul(x, y)
print(f"æµ‹è¯•å¼ é‡è®¾å¤‡: {z.device}")
print("GPUç¯å¢ƒé…ç½®æˆåŠŸï¼")
```

è¿è¡Œï¼š`python test_gpu.py`ï¼Œç¡®è®¤è¾“å‡ºæ˜¾ç¤º `cuda:0`ã€‚

---

## 5. æ•°æ®ç®¡ç†ä¸ä¼ è¾“

### 5.1 å¤§æ–‡ä»¶ä¼ è¾“æ–¹æ¡ˆ

#### æ–¹æ¡ˆAï¼šOSSå¯¹è±¡å­˜å‚¨ï¼ˆæ¨è>10GBï¼‰

**é˜¿é‡Œäº‘OSSé…ç½®**ï¼š

bash

å¤åˆ¶

```bash
# å®‰è£…ossutil
wget http://gosspublic.alicdn.com/ossutil/1.7.14/ossutil64
chmod 755 ossutil64
./ossutil64 config

# ä¸Šä¼ æœ¬åœ°æ•°æ®åˆ°OSS
./ossutil64 cp -r /æœ¬åœ°æ•°æ®è·¯å¾„ oss://ä½ çš„bucketåç§°/data/

# æœåŠ¡å™¨ä¸‹è½½
./ossutil64 cp -r oss://ä½ çš„bucketåç§°/data/ ~/projects/data/
```

#### æ–¹æ¡ˆBï¼šSCPå‘½ä»¤è¡Œï¼ˆé€‚åˆ<10GBï¼‰

bash

å¤åˆ¶

```bash
# æœ¬åœ°â†’æœåŠ¡å™¨
scp -P 12345 -r ./dataset root@connect.westb.seetacloud.com:~/projects/data/

# æœåŠ¡å™¨â†’æœ¬åœ°ï¼ˆä¸‹è½½ç»“æœï¼‰
scp -P 12345 root@connect.westb.seetacloud.com:~/projects/outputs/ ./results/
```

#### æ–¹æ¡ˆCï¼šRsyncå¢é‡åŒæ­¥ï¼ˆé€‚åˆé¢‘ç¹æ›´æ–°ï¼‰

bash

å¤åˆ¶

```bash
# æœ¬åœ°æ‰§è¡Œï¼Œå¢é‡åŒæ­¥ä»£ç 
rsync -avz --progress -e "ssh -p 12345" ./src/ root@connect.westb.seetacloud.com:~/projects/src/
```

### 5.2 æ•°æ®ç›®å½•è§„èŒƒ

bash

å¤åˆ¶

```bash
~/projects/drug-repurposing/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # åŸå§‹æ•°æ®ï¼ˆä¸åŠ å…¥gitï¼‰
â”‚   â”œâ”€â”€ processed/        # é¢„å¤„ç†åçš„æ•°æ®
â”‚   â””â”€â”€ external/         # å¤–éƒ¨æ•°æ®é›†
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/           # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ data/             # æ•°æ®åŠ è½½å™¨
â”‚   â””â”€â”€ utils/            # å·¥å…·å‡½æ•°
â”œâ”€â”€ configs/              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ notebooks/            # æ¢ç´¢æ€§åˆ†æ
â”œâ”€â”€ outputs/              # è®­ç»ƒè¾“å‡ºï¼ˆæƒé‡ã€æ—¥å¿—ï¼‰
â””â”€â”€ scripts/              # è®­ç»ƒè„šæœ¬
```

---

## 6. è®­ç»ƒæµç¨‹ä¸ä»£ç è§„èŒƒ

### 6.1 æ ‡å‡†è®­ç»ƒè„šæœ¬ç»“æ„

åˆ›å»º `src/train.py`ï¼š

Python

å¤åˆ¶

```python
import os
import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import logging
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser(description='Drug Response Prediction')
    parser.add_argument('--data_path', type=str, default='~/projects/data/processed')
    parser.add_argument('--batch_size', type=int, default=256)
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--lr', type=float, default=1e-3)
    parser.add_argument('--output_dir', type=str, default='~/projects/outputs')
    parser.add_argument('--num_workers', type=int, default=4)
    return parser.parse_args()

def main():
    args = parse_args()
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    exp_name = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    output_dir = os.path.join(args.output_dir, exp_name)
    os.makedirs(output_dir, exist_ok=True)
    
    # TensorBoard
    writer = SummaryWriter(os.path.join(output_dir, 'logs'))
    
    # æ•°æ®åŠ è½½ï¼ˆç¤ºä¾‹ï¼‰
    # train_loader = DataLoader(..., num_workers=args.num_workers, pin_memory=True)
    
    # æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    model = YourModel().to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        model.train()
        total_loss = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if batch_idx % 10 == 0:
                logger.info(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}")
        
        avg_loss = total_loss / len(train_loader)
        writer.add_scalar('Loss/train', avg_loss, epoch)
        scheduler.step(avg_loss)
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % 10 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, os.path.join(output_dir, f'checkpoint_epoch_{epoch}.pt'))
    
    writer.close()
    logger.info(f"è®­ç»ƒå®Œæˆï¼Œè¾“å‡ºä¿å­˜è‡³: {output_dir}")

if __name__ == '__main__':
    main()
```

### 6.2 åå°è¿è¡Œä¸æŒä¹…åŒ–

**ä½¿ç”¨tmuxä¿æŒä¼šè¯**ï¼ˆé˜²æ­¢SSHæ–­å¼€è®­ç»ƒä¸­æ–­ï¼‰ï¼š

bash

å¤åˆ¶

```bash
# å®‰è£…tmux
apt-get update && apt-get install -y tmux

# åˆ›å»ºæ–°ä¼šè¯
tmux new -s training

# åœ¨tmuxä¸­è¿è¡Œè®­ç»ƒ
cd ~/projects/drug-repurposing
python src/train.py --epochs 100 --batch_size 128

# åˆ†ç¦»ä¼šè¯ï¼ˆåå°è¿è¡Œï¼‰ï¼šæŒ‰ Ctrl+B ç„¶åæŒ‰ D
# é‡æ–°è¿æ¥ï¼štmux attach -t training
# æŸ¥çœ‹æ‰€æœ‰ä¼šè¯ï¼štmux ls
```

**ä½¿ç”¨nohupï¼ˆç®€å•æ–¹æ¡ˆï¼‰**ï¼š

bash

å¤åˆ¶

```bash
nohup python src/train.py > train.log 2>&1 &
# æŸ¥çœ‹æ—¥å¿—ï¼štail -f train.log
# æŸ¥æ‰¾è¿›ç¨‹ï¼šps aux | grep train.py
# åœæ­¢è¿›ç¨‹ï¼škill è¿›ç¨‹ID
```

---

## 7. ç›‘æ§ã€è°ƒè¯•ä¸æŒä¹…åŒ–

### 7.1 å®æ—¶ç›‘æ§GPUçŠ¶æ€

bash

å¤åˆ¶

```bash
# æŒç»­ç›‘æ§ï¼ˆæ¯2ç§’åˆ·æ–°ï¼‰
watch -n 2 nvidia-smi

# æˆ–å®‰è£…nvtopï¼ˆæ›´ç¾è§‚ï¼‰
apt-get install nvtop
nvtop
```

### 7.2 TensorBoardå¯è§†åŒ–

bash

å¤åˆ¶

```bash
# æœåŠ¡å™¨ç«¯å¯åŠ¨ï¼ˆæŒ‡å®šç«¯å£ï¼‰
tensorboard --logdir=~/projects/outputs --port=6006 --bind_all

# æœ¬åœ°ç«¯å£è½¬å‘ï¼ˆåœ¨æœ¬åœ°ç»ˆç«¯æ‰§è¡Œï¼‰
ssh -p 12345 -L 6006:localhost:6006 root@connect.westb.seetacloud.com

# æœ¬åœ°æµè§ˆå™¨è®¿é—®
http://localhost:6006
```

### 7.3 è¿œç¨‹è°ƒè¯•é…ç½®ï¼ˆVSCodeï¼‰

åˆ›å»º `.vscode/launch.json`ï¼š

JSON

å¤åˆ¶

```json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Train",
            "type": "python",
            "request": "launch",
            "program": "${workspaceFolder}/src/train.py",
            "console": "integratedTerminal",
            "args": ["--epochs", "10", "--batch_size", "64"],
            "env": {
                "CUDA_VISIBLE_DEVICES": "0"
            }
        }
    ]
}
```

æŒ‰F5å³å¯å¯åŠ¨è°ƒè¯•ï¼Œæ”¯æŒæ–­ç‚¹ã€å˜é‡æŸ¥çœ‹ã€å•æ­¥æ‰§è¡Œã€‚

### 7.4 æ¨¡å‹ä¸æ•°æ®å¤‡ä»½

**è‡ªåŠ¨ä¸Šä¼ æ£€æŸ¥ç‚¹åˆ°OSS**ï¼š

Python

å¤åˆ¶

```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ 
import subprocess

def upload_checkpoint(local_path, oss_path):
    cmd = f"ossutil cp {local_path} {oss_path}"
    subprocess.call(cmd, shell=True)

# æ¯ä¿å­˜ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œå¼‚æ­¥ä¸Šä¼ 
upload_checkpoint('checkpoint.pt', 'oss://your-bucket/checkpoints/')
```

---

## 8. æˆæœ¬ä¼˜åŒ–ä¸èµ„æºç®¡ç†

### 8.1 è‡ªåŠ¨å…³æœºè„šæœ¬ï¼ˆé˜²æ­¢ç©ºè½¬çƒ§é’±ï¼‰

åˆ›å»º `scripts/auto_shutdown.py`ï¼š

Python

å¤åˆ¶

```python
import os
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_gpu_idle():
    """æ£€æŸ¥GPUæ˜¯å¦ç©ºé—²è¶…è¿‡30åˆ†é’Ÿ"""
    import subprocess
    result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                          capture_output=True, text=True)
    utilization = int(result.stdout.strip())
    return utilization < 10  # åˆ©ç”¨ç‡ä½äº10%è§†ä¸ºç©ºé—²

def main():
    idle_time = 0
    while True:
        if check_gpu_idle():
            idle_time += 5
            logger.info(f"GPUç©ºé—² {idle_time} åˆ†é’Ÿ")
            if idle_time >= 30:
                logger.warning("GPUç©ºé—²è¶…è¿‡30åˆ†é’Ÿï¼Œå³å°†å…³æœº")
                os.system("shutdown -h now")  # éœ€è¦rootæƒé™
        else:
            idle_time = 0
        time.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

if __name__ == '__main__':
    main()
```

### 8.2 æ··åˆç²¾åº¦è®­ç»ƒï¼ˆèŠ‚çœæ˜¾å­˜+åŠ é€Ÿï¼‰

Python

å¤åˆ¶

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for data, target in train_loader:
    data, target = data.cuda(), target.cuda()
    
    optimizer.zero_grad()
    
    # è‡ªåŠ¨æ··åˆç²¾åº¦
    with autocast():
        output = model(data)
        loss = criterion(output, target)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### 8.3 å¤šGPUè®­ç»ƒï¼ˆDataParallel/DistributedDataParallelï¼‰

Python

å¤åˆ¶

```python
# å•æœºå¤šGPUï¼ˆç®€å•ç‰ˆï¼‰
if torch.cuda.device_count() > 1:
    logger.info(f"ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPU")
    model = nn.DataParallel(model)

model = model.cuda()
```

---

## é™„å½•ï¼šå¿«é€Ÿæ£€æŸ¥æ¸…å•

**ç§Ÿç”¨å‰**ï¼š

- [ ] ç¡®è®¤CUDAç‰ˆæœ¬ä¸PyTorchå…¼å®¹
    
- [ ] æ£€æŸ¥GPUæ˜¾å­˜æ˜¯å¦æ»¡è¶³æ¨¡å‹éœ€æ±‚ï¼ˆBatch Sizeæµ‹è¯•ï¼‰
    
- [ ] ç¡®è®¤ç£ç›˜ç©ºé—´è¶³å¤Ÿï¼ˆæ•°æ®é›†+æ¨¡å‹æ£€æŸ¥ç‚¹ï¼‰
    

**é…ç½®æ—¶**ï¼š

- [ ] SSHå…å¯†ç™»å½•é…ç½®å®Œæˆ
    
- [ ] VSCode Remoteè¿æ¥æˆåŠŸ
    
- [ ] `nvidia-smi`æ˜¾ç¤ºæ­£å¸¸
    
- [ ] PyTorchæµ‹è¯•è„šæœ¬é€šè¿‡
    

**è®­ç»ƒå‰**ï¼š

- [ ] æ•°æ®å·²ä¸Šä¼ åˆ°æœåŠ¡å™¨
    
- [ ] ä»£ç å·²ç‰ˆæœ¬æ§åˆ¶ï¼ˆGitï¼‰
    
- [ ] è¾“å‡ºç›®å½•å·²åˆ›å»º
    
- [ ] TensorBoardç«¯å£å·²è½¬å‘
    

**è®­ç»ƒä¸­**ï¼š

- [ ] ä½¿ç”¨tmux/nohupä¿æŒä¼šè¯
    
- [ ] ç›‘æ§æ˜¾å­˜å ç”¨ï¼ˆé¿å…OOMï¼‰
    
- [ ] å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
    
- [ ] è®°å½•å®éªŒå‚æ•°ï¼ˆæ¨èWandBæˆ–MLflowï¼‰
    

---

**ä¸‹ä¸€æ­¥å»ºè®®**ï¼š

1. å…ˆç”¨AutoDLå¼€ä¸€å°RTX 3090å®ä¾‹ï¼ŒæŒ‰ç¬¬3ç« é…ç½®å¥½VSCodeè¿œç¨‹ç¯å¢ƒ
    
2. è·‘é€šç¬¬4ç« çš„GPUæµ‹è¯•è„šæœ¬
    
3. å°†ä½ çš„è¯ç‰©æ‰°åŠ¨é¡¹ç›®ä»£ç è¿ç§»ä¸Šå»ï¼Œç”¨ç¬¬6ç« çš„æ¨¡æ¿æ”¹é€ è®­ç»ƒè„šæœ¬
    
4. é…ç½®ç¬¬7ç« çš„TensorBoardç›‘æ§ï¼Œå¼€å§‹ç¬¬ä¸€æ¬¡äº‘ç«¯è®­ç»ƒ
    

éœ€è¦æˆ‘é’ˆå¯¹è¯ç‰©æ‰°åŠ¨ï¼ˆDrug Perturbationï¼‰é¡¹ç›®æä¾›ç‰¹å®šçš„æ•°æ®åŠ è½½å™¨æˆ–æ¨¡å‹ç»“æ„ç¤ºä¾‹å—ï¼Ÿ